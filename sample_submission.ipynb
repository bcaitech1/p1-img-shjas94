{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Thu Apr  8 08:19:25 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P40           On   | 00000000:00:05.0 Off |                  Off |\n| N/A   51C    P0   156W / 250W |   6693MiB / 24451MiB |     79%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import glob\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from adamp import AdamP\n",
    "from madgrad import MADGRAD\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "from utils import GetPathNLabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "train_image_dir = '/opt/ml/input/data/train/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(train_dir, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_label = []\n",
    "for i in range(len(df)):\n",
    "    temp_gender = df.iloc[i].gender\n",
    "    temp_age = df.iloc[i].age\n",
    "    gender_label = 0 if temp_gender == 'male' else 3\n",
    "    age_label = 0 if temp_age < 30 else 1 if temp_age >= 30 and temp_age < 60 else 2\n",
    "    temp_label.append(gender_label + age_label)\n",
    "\n",
    "df_label = pd.Series(temp_label)\n",
    "df_label_add = pd.concat((df, df_label), axis=1)\n",
    "df_label_add.rename(columns={0:'label'}, inplace=True)\n",
    "temp_label = df_label_add['label'].to_list()\n",
    "temp_folder = df_label_add['path'].to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "## 1. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EffNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform, aug=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "        self.aug = aug\n",
    "    def __getitem__(self, index):\n",
    "        # image = Image.open(self.img_paths[index])\n",
    "        image = cv2.imread(self.img_paths[index], cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # image = (image/255.)\n",
    "        img_ycrcb = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        \n",
    "        ycrcb_planes = cv2.split(img_ycrcb)\n",
    "        # 밝기 성분에 대해서만 histogram equalization\n",
    "        ycrcb_planes[0] = cv2.equalizeHist(ycrcb_planes[0])\n",
    "        dst_ycrcb = cv2.merge(ycrcb_planes)\n",
    "        dst = cv2.cvtColor(dst_ycrcb, cv2.COLOR_YCrCb2RGB)\n",
    "        dst = (dst/255.).astype('float32')\n",
    "\n",
    "        if self.aug:\n",
    "            dst = self.aug(image=dst)['image']\n",
    "        if self.transform:\n",
    "            dst = self.transform(dst)\n",
    "        \n",
    "        return dst\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "source": [
    "## Train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MaskDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Loss(nn.Module):\n",
    "    def __init__(self, classes=18, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.ndim == 2\n",
    "        assert y_true.ndim == 1\n",
    "        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n",
    "        return 1 - f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )\n"
   ]
  },
  {
   "source": [
    "## MIXUP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-d131b97f7897>, line 21)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-d131b97f7897>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    A.HueSaturationValue(p=0.3),\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model_save_path = os.path.join(os.getcwd(), 'model', 'day11/')\n",
    "models = []\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_models = []\n",
    "for fold_index, (train_idx, val_idx) in enumerate(skfold.split(temp_folder, temp_label)):\n",
    "    # if fold_index == 0 or fold_index == 1 or fold_index == 2 or fold_index == 3:\n",
    "    #     continue\n",
    "    print(f'[fold: {fold_index}]')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    augmentations = A.Compose([\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        # A.Rotate(limit=20, p=0.3),\n",
    "        # A.HorizontalFlip(p=0.3),\n",
    "        # A.VerticalFlip(p=0.3),\n",
    "        # A.IAAPiecewiseAffine(p=0.3),\n",
    "        # A.RandomRotate90(p=0.2),\n",
    "        # A.RandomGamma(p=0.2),\n",
    "        # A.GaussNoise(),\n",
    "        # A.GaussianBlur(blur_limit=(3,5)),\n",
    "        A.HueSaturationValue(p=0.3),\n",
    "        # A.RGBShift()\n",
    "    ])\n",
    "    train_dirs = [temp_folder[i] for i in train_idx]\n",
    "    val_dirs = [temp_folder[i] for i in val_idx]\n",
    "    \n",
    "\n",
    "\n",
    "    ############################################################\n",
    "    # train_images = [images[i] for i in train_idx]\n",
    "    # train_labels = [labels[i] for i in train_idx]\n",
    "\n",
    "    # val_images = [images[i] for i in val_idx]\n",
    "    # val_labels = [labels[i] for i in val_idx]\n",
    "    train_path = GetPathNLabel(train_dirs)\n",
    "    val_path = GetPathNLabel(val_dirs)\n",
    "    train_images, train_labels = train_path.call()\n",
    "    val_images, val_labels = val_path.call()\n",
    "    \n",
    "\n",
    "    train_dataset = MaskDataset(train_images, train_labels, augmentations=augmentations)\n",
    "    val_dataset = MaskDataset(val_images, val_labels)\n",
    "    train_data_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=3)\n",
    "    val_data_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=3)\n",
    "    \n",
    "    model = EffNet()\n",
    "    # model._dropout = nn.Dropout(p=0.7, inplace=False)\n",
    "    model.to(device)\n",
    "    # optimizer = AdamP(model.parameters(), lr=0.0001)\n",
    "    optimizer = MADGRAD(model.parameters(), lr=0.0001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=4, eta_min=0.000005)\n",
    "    criterion1 = FocalLoss(gamma=5)\n",
    "    criterion2 = F1Loss()\n",
    "    #################\n",
    "    valid_f1_max = 0\n",
    "    valid_loss_min = float('inf')\n",
    "    best_model = None\n",
    "    for epoch in range(15):\n",
    "            # train_acc_list = []\n",
    "            train_f1_list = []\n",
    "            train_loss_list = []\n",
    "            with tqdm(train_data_loader,\n",
    "                    total=train_data_loader.__len__(), \n",
    "                    unit=\"batch\") as train_bar:\n",
    "                for sample in train_bar:\n",
    "                    train_bar.set_description(f\"Train Epoch {epoch}\")\n",
    "                \n",
    "                    optimizer.zero_grad()\n",
    "                    x_train, y_train = sample['image'], sample['label']\n",
    "                    x_train = x_train.to(device)\n",
    "                    y_train = y_train.to(device)\n",
    "                    model.train()\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        preds  = model(x_train)\n",
    "                        loss = (0.5*criterion1(preds, y_train)) + (0.5*criterion2(preds, y_train))\n",
    "                        preds = preds.argmax(dim=-1)\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        preds  = preds.cpu().detach().numpy()\n",
    "                        y_train = y_train.cpu().detach().numpy()\n",
    "                        # batch_acc = (y_train == preds).mean()    \n",
    "                        # train_acc_list.append(batch_acc)\n",
    "                        # train_acc = np.mean(train_acc_list)\n",
    "                        batch_f1 = f1_score(y_train, preds, average='macro')\n",
    "                        train_f1_list.append(batch_f1)\n",
    "                        train_loss_list.append(loss.item())\n",
    "                        \n",
    "\n",
    "                        train_f1 = np.mean(train_f1_list)\n",
    "                        train_loss = np.mean(train_loss_list)\n",
    "                        train_bar.set_postfix(train_loss= train_loss,\n",
    "                                        train_f1 = train_f1)\n",
    "                    \n",
    "                    \n",
    "\n",
    "            # valid_acc_list = []\n",
    "            valid_f1_list = []\n",
    "            valid_loss_list = []\n",
    "            with tqdm(val_data_loader,\n",
    "                    total=val_data_loader.__len__(),\n",
    "                    unit=\"batch\") as valid_bar:\n",
    "                for sample in valid_bar:\n",
    "                    valid_bar.set_description(f\"Valid Epoch {epoch}\")\n",
    "                    optimizer.zero_grad()\n",
    "                    x_val, y_val = sample['image'], sample['label']\n",
    "                    x_val = x_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        preds = model(x_val)\n",
    "                        valid_loss = (0.5*criterion1(preds, y_val)) + (0.5*criterion2(preds, y_val))\n",
    "                        preds = preds.argmax(dim=-1)\n",
    "\n",
    "                        preds  = preds.cpu().detach().numpy()\n",
    "                        y_val = y_val.cpu().detach().numpy()\n",
    "                        # batch_acc = (y_val == preds).mean()\n",
    "                        # valid_acc_list.append(batch_acc)\n",
    "                        batch_f1 = f1_score(y_val, preds, average='macro')\n",
    "                        valid_f1_list.append(batch_f1)\n",
    "                        valid_loss_list.append(valid_loss.item())\n",
    "                        # valid_acc = np.mean(valid_acc_list)\n",
    "                        valid_f1 = np.mean(valid_f1_list)\n",
    "                        valid_loss = np.mean(valid_loss_list)\n",
    "                        valid_bar.set_postfix(valid_loss = valid_loss,\n",
    "                                        valid_f1 = valid_f1)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                \n",
    "            lr_scheduler.step()\n",
    "\n",
    "            if valid_f1_max < valid_f1:\n",
    "                print()\n",
    "                print(f\"best model changed!!, previous: {valid_f1_max} VS current: {valid_f1}\")\n",
    "                valid_f1_max = valid_f1\n",
    "                best_model = model\n",
    "                best_epoch = epoch\n",
    "        \n",
    "    model_name = \"Effnetb3\"\n",
    "    path = model_save_path\n",
    "    torch.save(best_model, f'{model_save_path}{model_name}_{fold_index}_{valid_f1_max:2.4f}_epoch_{best_epoch}.pth')\n",
    "    models.append(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "# models.append(torch.load('./model/day9/Effnetb3_0_0.7576_epoch_9.pth'))\n",
    "# models.append(torch.load('./model/day9/Effnetb3_1_0.7506_epoch_8.pth'))\n",
    "# models.append(torch.load('./model/day9/Effnetb3_2_0.7502_epoch_11.pth'))\n",
    "# models.append(torch.load('./model/day9/Effnetb3_3_0.7506_epoch_8.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/opt/ml/input/data/eval/images/cbc5c6e168e63498590db46022617123f1fe1268.jpg',\n",
       " '/opt/ml/input/data/eval/images/0e72482bf56b3581c081f7da2a6180b8792c7089.jpg',\n",
       " '/opt/ml/input/data/eval/images/b549040c49190cedc41327748aeb197c1670f14d.jpg',\n",
       " '/opt/ml/input/data/eval/images/4f9cb2a045c6d5b9e50ad3459ea7b791eb6e18bc.jpg',\n",
       " '/opt/ml/input/data/eval/images/248428d9a4a5b6229a7081c32851b90cb8d38d0c.jpg',\n",
       " '/opt/ml/input/data/eval/images/7953c2e6e983bad91b89a0e4ad7eb69e6e43e56b.jpg',\n",
       " '/opt/ml/input/data/eval/images/1903db7dcc1262d0441677afa422e6a8371e5b37.jpg',\n",
       " '/opt/ml/input/data/eval/images/441419a874f4d031cd576850b68539ca7d35bedf.jpg',\n",
       " '/opt/ml/input/data/eval/images/388856cd1ef99b1918273a827a75f2aff2478321.jpg',\n",
       " '/opt/ml/input/data/eval/images/795ba8ccc769a3f9da6a897f75df6706b729345b.jpg']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "image_paths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "preds = []\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "\n",
    "for model in models:\n",
    "    transform = transforms.Compose([\n",
    "        # Resize((512, 384), Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.CenterCrop((300, 256)),\n",
    "\n",
    "        transforms.ToTensor(),\n",
    "        Normalize(mean=(0.560, 0.524, 0.501), std=(0.233, 0.243, 0.245)),\n",
    "        # transforms.ToTensor(),\n",
    "        # transforms.ToPILImage(),\n",
    "        # transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=3,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "    device = torch.device('cuda')\n",
    "    # model = torch.load('./model/Effnetb2_0.0788_epoch_2.pth')\n",
    "    model.eval()\n",
    "\n",
    "    # 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "    all_predictions = []\n",
    "    for images in loader:\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            pred = model(images)\n",
    "            # preds.append(pred)\n",
    "            # pred = pred.argmax(dim=-1)\n",
    "            all_predictions.extend(pred.cpu().numpy())\n",
    "            # print(len(all_predictions))\n",
    "\n",
    "    preds.append(all_predictions)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_np = np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7, 12600, 18)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "pred_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_np_sum = pred_np.sum(axis=0)\n",
    "pred_np_mean = np.mean(pred_np_sum, axis=-1)\n",
    "final = np.argmax(pred_np_sum, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "submission['ans'] = final\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission_final4.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}